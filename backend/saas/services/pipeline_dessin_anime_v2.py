#!/usr/bin/env python3
"""
üé¨ Pipeline de G√©n√©ration de Dessin Anim√© IA
Impl√©mentation compl√®te selon les sp√©cifications fonctionnelles
"""

import json
import os
import asyncio
import aiohttp
import subprocess
import requests  # Ajout pour Stability AI
import base64  # Pour d√©coder les vid√©os base64
from typing import List, Dict, Any
from pathlib import Path
import time
import openai
from openai import AsyncOpenAI
from PIL import Image  # Pour redimensionner les images

class DessinAnimePipeline:
    """Pipeline modulaire pour g√©n√©ration de dessins anim√©s IA"""
    
    def __init__(self, openai_api_key: str, stability_api_key: str):
        self.openai_client = AsyncOpenAI(api_key=openai_api_key)
        self.stability_api_key = stability_api_key
        self.mode = "demo"  # Mode par d√©faut
        
        # Configuration
        self.cache_dir = Path("cache/animations")
        self.clips_dir = self.cache_dir / "clips"
        self.final_dir = self.cache_dir / "final"
        
        # Cr√©er les r√©pertoires
        for dir_path in [self.cache_dir, self.clips_dir, self.final_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    async def decouper_histoire_en_scenes(self, histoire: str, duree_totale: int) -> List[Dict[str, Any]]:
        """
        √âtape 1: D√©couper l'histoire en 8-12 sc√®nes visuelles
        """
        print(f"üé¨ √âtape 1: D√©coupage de l'histoire en sc√®nes (dur√©e cible: {duree_totale}s)")
        
        # Calculer le nombre de sc√®nes optimal
        nb_scenes = max(8, min(12, duree_totale // 20))  # 20s par sc√®ne en moyenne
        duree_par_scene = duree_totale // nb_scenes
        
        prompt = f"""Tu es un expert en narration visuelle pour dessins anim√©s enfants.

HISTOIRE √Ä D√âCOUPER:
{histoire}

CONTRAINTES:
- D√©couper en {nb_scenes} sc√®nes visuelles distinctes
- Dur√©e cible par sc√®ne: {duree_par_scene} secondes
- Chaque sc√®ne doit √™tre visuellement marquante et adapt√©e aux enfants
- Privil√©gier les actions et mouvements visibles
- √âviter les dialogues longs, se concentrer sur l'action visuelle

STRUCTURE DE R√âPONSE JSON EXACTE:
{{
    "total_scenes": {nb_scenes},
    "duree_totale_cible": {duree_totale},
    "scenes": [
        {{
            "id": 1,
            "description": "Description visuelle pr√©cise de la sc√®ne",
            "duree_estimee": {duree_par_scene},
            "action_principale": "Action ou mouvement principal",
            "personnages": ["personnage1", "personnage2"],
            "decor": "Description du d√©cor/environnement"
        }}
    ]
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content.strip()
            
            # Nettoyer le JSON
            if content.startswith("```json"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            scenes_data = json.loads(content)
            print(f"‚úÖ {scenes_data['total_scenes']} sc√®nes g√©n√©r√©es")
            
            return scenes_data['scenes']
            
        except Exception as e:
            print(f"‚ùå Erreur d√©coupage: {e}")
            # Fallback simple
            return [
                {
                    "id": i+1,
                    "description": f"Sc√®ne {i+1} de l'histoire",
                    "duree_estimee": duree_par_scene,
                    "action_principale": "Action narrative",
                    "personnages": ["personnage principal"],
                    "decor": "Environnement de l'histoire"
                }
                for i in range(nb_scenes)
            ]
    
    async def definir_style_et_seeds(self, scenes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        √âtape 2: D√©finir style visuel homog√®ne et seeds pour coh√©rence
        """
        print("üé® √âtape 2: D√©finition du style visuel et seeds")
        
        # Extraire personnages et lieux r√©currents
        tous_personnages = set()
        tous_lieux = set()
        
        for scene in scenes:
            tous_personnages.update(scene.get('personnages', []))
            tous_lieux.add(scene.get('decor', ''))
        
        prompt = f"""Tu es un directeur artistique pour dessins anim√©s enfants.

SC√àNES √Ä STYLISER:
{json.dumps(scenes, indent=2, ensure_ascii=False)}

OBJECTIF:
Cr√©er un style visuel coh√©rent et des seeds pour garantir la consistance entre les clips.

STRUCTURE DE R√âPONSE JSON EXACTE:
{{
    "style_global": "description compl√®te du style visuel (couleurs, ambiance, technique d'animation)",
    "palette_couleurs": ["#couleur1", "#couleur2", "#couleur3"],
    "ambiance": "magique/aventureuse/douce/etc",
    "seeds": {{
        "style_principal": 12345,
        "environnement_principal": 23456
    }},
    "qualite_tags": ["high quality", "detailed", "colorful", "child-friendly"],
    "style_prompt_base": "prompt de base √† injecter dans chaque g√©n√©ration"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.6,
                max_tokens=1500
            )
            
            content = response.choices[0].message.content.strip()
            if content.startswith("```json"):
                content = content.replace("```json", "").replace("```", "").strip()
            
            style_data = json.loads(content)
            print(f"‚úÖ Style d√©fini: {style_data['ambiance']}")
            
            return style_data
            
        except Exception as e:
            print(f"‚ùå Erreur style: {e}")
            # Fallback
            return {
                "style_global": "Dessin anim√© color√© et lumineux adapt√© aux enfants",
                "palette_couleurs": ["#FFB6C1", "#87CEEB", "#98FB98"],
                "ambiance": "magique",
                "seeds": {"style_principal": 12345, "environnement_principal": 23456},
                "qualite_tags": ["high quality", "detailed", "colorful", "child-friendly"],
                "style_prompt_base": "beautiful animated style, child-friendly, colorful"
            }
    
    async def generer_prompts_videos(self, scenes: List[Dict[str, Any]], style: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        √âtape 3: G√©n√©rer prompts text-to-video optimis√©s pour SD3-Turbo
        """
        print("üìù √âtape 3: G√©n√©ration des prompts vid√©o")
        
        prompts = []
        
        for scene in scenes:
            prompt = f"""You are an expert in text-to-video prompts for Stability AI.

SCENE TO CONVERT:
{json.dumps(scene, indent=2, ensure_ascii=False)}

IMPOSED STYLE:
{json.dumps(style, indent=2, ensure_ascii=False)}

OBJECTIVE:
Create an optimized prompt for SD3-Turbo that generates a {scene['duree_estimee']}s video clip.

EXACT JSON RESPONSE STRUCTURE:
{{
    "scene_id": {scene['id']},
    "prompt_text": "complete prompt optimized for text-to-video IN ENGLISH ONLY",
    "prompt_text_english": "same prompt in English for Stability AI",
    "duration": {scene['duree_estimee']},
    "seed": {style['seeds']['style_principal']},
    "camera_movement": "camera movement description",
    "visual_elements": ["element1", "element2"],
    "style_tags": {json.dumps(style['qualite_tags'])}
}}

PROMPT RULES:
- WRITE EVERYTHING IN ENGLISH (Stability AI requirement)
- Maximum 500 characters
- Describe action, movement, atmosphere
- Include camera movement (zoom, pan, tracking)
- Visual style naturally integrated
- Optimized for text-to-video, not static image
- Child-friendly cartoon style
- Use words like: cartoon, animated, colorful, cute, friendly"""

            try:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=800
                )
                
                content = response.choices[0].message.content.strip()
                if content.startswith("```json"):
                    content = content.replace("```json", "").replace("```", "").strip()
                
                prompt_data = json.loads(content)
                prompts.append(prompt_data)
                
            except Exception as e:
                print(f"‚ùå Erreur prompt sc√®ne {scene['id']}: {e}")
                # Fallback
                prompts.append({
                    "scene_id": scene['id'],
                    "prompt_text": f"{style['style_prompt_base']}, {scene['description']}, {scene['action_principale']}",
                    "duration": scene['duree_estimee'],
                    "seed": style['seeds']['style_principal'],
                    "camera_movement": "smooth camera movement",
                    "visual_elements": scene.get('personnages', []),
                    "style_tags": style['qualite_tags']
                })
        
        print(f"‚úÖ {len(prompts)} prompts g√©n√©r√©s")
        return prompts
    
    async def generer_clips_video(self, prompts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        √âtape 4: G√©n√©rer les clips vid√©o via Stability AI
        """
        if self.mode == "production":
            print("üé• √âtape 4: G√©n√©ration des clips vid√©o avec Stability AI (MODE PRODUCTION)")
        else:
            print("üé• √âtape 4: G√©n√©ration des clips vid√©o (MODE D√âMO)")
        
        clips = []
        
        for prompt_data in prompts:
            print(f"   G√©n√©ration clip sc√®ne {prompt_data['scene_id']}...")
            
            if self.mode == "production":
                try:
                    # Mode production : vraie g√©n√©ration Stability AI
                    print(f"      üé¨ Tentative g√©n√©ration Stability AI...")
                    clip_result = await self._generer_clip_stability_ai(prompt_data)
                    
                    if clip_result:
                        clips.append(clip_result)
                        print(f"      ‚úÖ Clip Stability AI g√©n√©r√© avec succ√®s")
                    else:
                        print(f"      ‚ö†Ô∏è Clip Stability AI vide - fallback vers d√©mo")
                        clip_result = await self._generer_clip_demo(prompt_data)
                        clips.append(clip_result)
                    
                    # Pause entre les g√©n√©rations pour √©viter la rate limit
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    print(f"      ‚ùå ERREUR Stability AI pour sc√®ne {prompt_data['scene_id']}: {e}")
                    print(f"      üìã Type d'erreur: {type(e).__name__}")
                    import traceback
                    print(f"      üìù D√©tails: {traceback.format_exc()}")
                    print("      üîÑ Fallback vers g√©n√©ration de d√©mo...")
                    
                    # Fallback vers d√©mo en cas d'erreur
                    clip_result = await self._generer_clip_demo(prompt_data)
                    clips.append(clip_result)
            else:
                # Mode d√©mo : g√©n√©ration rapide avec images
                clip_result = await self._generer_clip_demo(prompt_data)
                clips.append(clip_result)
        
        print(f"‚úÖ {len(clips)} clips g√©n√©r√©s")
        return clips

    async def _generer_clip_stability_ai(self, prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        G√©n√©ration de clip vid√©o r√©elle via Stability AI
        """
        scene_id = prompt_data['scene_id']
        
        # Pour SD3-Turbo, nous devons d'abord g√©n√©rer une image, puis la convertir en vid√©o
        # √âtape 1: G√©n√©rer une image de base avec SD3
        image_result = await self._generer_image_sd3(prompt_data)
        
        if not image_result:
            raise Exception("√âchec g√©n√©ration image de base")
        
        # √âtape 2: Convertir l'image en vid√©o avec Stable Video Diffusion
        video_result = await self._image_to_video_stability(image_result, prompt_data)
        
        if not video_result:
            raise Exception("√âchec conversion image vers vid√©o")
        
        return {
            "scene_number": scene_id,
            "scene_id": scene_id,
            "video_path": video_result['video_path'],
            "video_url": video_result['video_url'],
            "image_url": image_result['image_url'],  # Image de base
            "thumbnail_url": image_result['image_url'],
            "status": "success",
            "statut": "generated",
            "duration": prompt_data['duration'],
            "prompt": prompt_data['prompt_text'],
            "generation_method": "stability_ai",
            "type": "real_video",
            "metadata": {
                "scene_id": scene_id,
                "image_generation": image_result,
                "video_generation": video_result,
                "timestamp": time.time()
            }
        }

    async def _generer_image_sd3(self, prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        G√©n√©rer une image de base avec SD3
        """
        print(f"      üñºÔ∏è G√©n√©ration image SD3 pour sc√®ne {prompt_data['scene_id']}")
        
        url = "https://api.stability.ai/v2beta/stable-image/generate/sd3"
        
        headers = {
            "Authorization": f"Bearer {self.stability_api_key}",
            "Accept": "image/*",
        }
        
        # Optimiser le prompt pour SD3 (utiliser la version anglaise)
        prompt_text = prompt_data.get('prompt_text_english', prompt_data.get('prompt_text', ''))
        optimized_prompt = f"{prompt_text}, single frame, high quality animation style, colorful, child-friendly"
        
        # SOLUTION: Utiliser requests pour Stability AI (fonctionne!)
        headers_req = {
            "Authorization": f"Bearer {self.stability_api_key}",
            "Accept": "image/*",
        }
        
        files = {
            "prompt": (None, optimized_prompt),
            "mode": (None, "text-to-image"),
            "model": (None, "sd3-medium"),
            "seed": (None, str(prompt_data.get('seed', 12345))),
            "output_format": (None, "png"),
            "width": (None, "1024"),
            "height": (None, "576")  # Format 16:9 compatible avec Stable Video
        }
        
        try:
            response = requests.post(url, headers=headers_req, files=files, timeout=60)
            
            print(f"        üìä Status SD3: {response.status_code}")
            
            if response.status_code == 200:
                # Sauvegarder l'image temporaire
                image_data = response.content
                temp_image_path = self.clips_dir / f"scene_{prompt_data['scene_id']}_temp.png"
                
                with open(temp_image_path, 'wb') as f:
                    f.write(image_data)
                
                # Redimensionner pour Stable Video (1024x576)
                with Image.open(temp_image_path) as img:
                    # Redimensionner en gardant le ratio et en centrant
                    resized_img = img.resize((1024, 576), Image.Resampling.LANCZOS)
                    
                    # Sauvegarder l'image redimensionn√©e
                    image_filename = f"scene_{prompt_data['scene_id']}_base.png"
                    image_path = self.clips_dir / image_filename
                    resized_img.save(image_path, "PNG")
                
                # Supprimer l'image temporaire
                temp_image_path.unlink(missing_ok=True)
                
                print(f"        ‚úÖ Image g√©n√©r√©e et redimensionn√©e: {image_filename} (1024x576)")
                
                return {
                    "image_path": str(image_path),
                    "image_url": f"/cache/animations/clips/{image_filename}",
                    "generation_id": f"sd3_{prompt_data['scene_id']}_{int(time.time())}"
                }
            else:
                error_text = response.text
                print(f"        ‚ùå Erreur SD3: {response.status_code} - {error_text}")
                return None
                
        except Exception as e:
            print(f"        üí• Exception requests: {e}")
            return None

    async def _image_to_video_stability(self, image_result: Dict[str, Any], prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convertir l'image en vid√©o avec Stable Video Diffusion
        """
        print(f"      üé¨ Conversion image‚Üívid√©o pour sc√®ne {prompt_data['scene_id']}")
        
        url = "https://api.stability.ai/v2beta/image-to-video"
        
        headers = {
            "Authorization": f"Bearer {self.stability_api_key}",
        }
        
        # Pr√©parer l'image pour upload
        image_path = image_result['image_path']
        
        # SOLUTION: Utiliser requests pour la conversion image‚Üívid√©o aussi
        try:
            with open(image_path, 'rb') as image_file:
                files = {
                    'image': ('input.png', image_file, 'image/png'),
                    'seed': (None, str(prompt_data.get('seed', 12345))),
                    'cfg_scale': (None, '2.5'),
                    'motion_bucket_id': (None, '127'),
                }
                
                response = requests.post(url, headers=headers, files=files, timeout=60)
                
                print(f"        üìä Status conversion: {response.status_code}")
                
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '')
                    response_text = response.text
                    
                    print(f"        üìä Content-Type: {content_type}")
                    print(f"        üìä Response length: {len(response_text)}")
                    print(f"        üìä Response starts with: {response_text[:50]}...")
                    
                    # V√©rifier si c'est du JSON
                    try:
                        result = response.json()
                        generation_id = result.get('id')
                        
                        if generation_id:
                            print(f"        üé¨ G√©n√©ration ID: {generation_id}")
                            # Attendre et r√©cup√©rer le r√©sultat
                            video_result = await self._wait_for_video_result(generation_id)
                            return video_result
                        else:
                            print(f"        ‚ùå Pas d'ID de g√©n√©ration re√ßu")
                            return None
                    except:
                        # Ce n'est pas du JSON valide
                        pass
                    
                    # V√©rifier si c'est une r√©ponse vid√©o en base64 ou binaire
                    if 'video/' in content_type or len(response.content) > 10000:
                        # R√©ponse directe avec vid√©o binaire
                        print(f"        ‚úÖ Vid√©o binaire re√ßue directement ({len(response.content)} bytes)")
                        
                        scene_id = prompt_data['scene_id']
                        video_filename = f"scene_{scene_id}_{int(time.time())}.mp4"
                        video_path = self.clips_dir / video_filename
                        
                        with open(video_path, 'wb') as f:
                            f.write(response.content)
                        
                        print(f"        üíæ Vid√©o sauvegard√©e: {video_filename}")
                        
                        return {
                            "video_path": str(video_path),
                            "video_url": f"/cache/animations/clips/{video_filename}",
                            "generation_id": f"direct_{scene_id}"
                        }
                    elif len(response_text) > 1000 and (response_text.startswith('UklGR') or response_text.startswith('//')):
                        # R√©ponse directe avec vid√©o en base64
                        print(f"        ‚úÖ Vid√©o base64 re√ßue directement ({len(response_text)} caract√®res)")
                        
                        try:
                            # D√©coder le base64
                            video_data = base64.b64decode(response_text)
                            
                            scene_id = prompt_data['scene_id']
                            video_filename = f"scene_{scene_id}_{int(time.time())}.mp4"
                            video_path = self.clips_dir / video_filename
                            
                            with open(video_path, 'wb') as f:
                                f.write(video_data)
                            
                            print(f"        üíæ Vid√©o d√©cod√©e et sauvegard√©e: {video_filename}")
                            
                            return {
                                "video_path": str(video_path),
                                "video_url": f"/cache/animations/clips/{video_filename}",
                                "generation_id": f"direct_{scene_id}"
                            }
                        except Exception as e:
                            print(f"        ‚ùå Erreur d√©codage base64: {e}")
                            return None
                    else:
                        print(f"        ‚ùì Type de contenu inattendu: {content_type}")
                        print(f"        ‚ùì Longueur: {len(response_text)}")
                        return None
                else:
                    error_text = response.text
                    print(f"        ‚ùå Erreur image-to-video: {response.status_code} - {error_text}")
                    return None
                    
        except Exception as e:
            print(f"        üí• Exception conversion: {e}")
            return None

    async def _wait_for_video_result(self, generation_id: str) -> Dict[str, Any]:
        """
        Attendre et r√©cup√©rer le r√©sultat de g√©n√©ration vid√©o
        """
        print(f"      ‚è≥ Attente g√©n√©ration vid√©o {generation_id}")
        
        url = f"https://api.stability.ai/v2beta/image-to-video/result/{generation_id}"
        headers = {
            "Authorization": f"Bearer {self.stability_api_key}",
            "Accept": "application/json"
        }
        
        max_attempts = 30  # Maximum 5 minutes d'attente
        attempt = 0
        
        async with aiohttp.ClientSession() as session:
            while attempt < max_attempts:
                await asyncio.sleep(10)  # Attendre 10 secondes entre chaque v√©rification
                attempt += 1
                
                async with session.get(url, headers=headers) as response:
                    print(f"      üìä Status check: {response.status}")
                    
                    if response.status == 200:
                        content_type = response.headers.get('content-type', '')
                        print(f"      üìä Content-Type: {content_type}")
                        
                        if 'application/json' in content_type:
                            result = await response.json()
                            print(f"      üìä JSON keys: {list(result.keys())}")
                            
                            # V√©rifier si c'est une URL de t√©l√©chargement
                            video_url = result.get('video')
                            if video_url:
                                print(f"      üîó Donn√©es vid√©o trouv√©es: {video_url[:50]}...")
                                
                                # V√©rifier si c'est une URL ou du base64
                                if video_url.startswith('http'):
                                    # C'est une URL, t√©l√©charger
                                    video_filename = f"scene_{generation_id}.mp4"
                                    video_path = self.clips_dir / video_filename
                                    
                                    async with session.get(video_url) as video_response:
                                        if video_response.status == 200:
                                            video_data = await video_response.read()
                                            with open(video_path, 'wb') as f:
                                                f.write(video_data)
                                            
                                            print(f"      ‚úÖ Vid√©o t√©l√©charg√©e: {video_filename}")
                                            
                                            return {
                                                "video_path": str(video_path),
                                                "video_url": f"/cache/animations/clips/{video_filename}",
                                                "generation_id": generation_id
                                            }
                                else:
                                    # C'est du base64, d√©coder directement
                                    try:
                                        video_data = base64.b64decode(video_url)
                                        
                                        video_filename = f"scene_{generation_id}.mp4"
                                        video_path = self.clips_dir / video_filename
                                        
                                        with open(video_path, 'wb') as f:
                                            f.write(video_data)
                                        
                                        print(f"      ‚úÖ Vid√©o base64 d√©cod√©e: {video_filename}")
                                        
                                        return {
                                            "video_path": str(video_path),
                                            "video_url": f"/cache/animations/clips/{video_filename}",
                                            "generation_id": generation_id
                                        }
                                    except Exception as e:
                                        print(f"      ‚ùå Erreur d√©codage base64: {e}")
                                        return None
                            
                            # V√©rifier si la vid√©o est directement en base64 dans la r√©ponse
                            elif 'video' in result or any(k for k in result.keys() if 'video' in k.lower()):
                                print(f"      üìπ Vid√©o trouv√©e dans JSON")
                                # Chercher le champ contenant la vid√©o
                                video_content = None
                                for key, value in result.items():
                                    if isinstance(value, str) and len(value) > 1000:
                                        video_content = value
                                        break
                                
                                if video_content:
                                    try:
                                        # Essayer de d√©coder le base64
                                        video_data = base64.b64decode(video_content)
                                        
                                        video_filename = f"scene_{generation_id}.mp4"
                                        video_path = self.clips_dir / video_filename
                                        
                                        with open(video_path, 'wb') as f:
                                            f.write(video_data)
                                        
                                        print(f"      ‚úÖ Vid√©o base64 d√©cod√©e: {video_filename}")
                                        
                                        return {
                                            "video_path": str(video_path),
                                            "video_url": f"/cache/animations/clips/{video_filename}",
                                            "generation_id": generation_id
                                        }
                                    except Exception as e:
                                        print(f"      ‚ùå Erreur d√©codage: {e}")
                            
                            # V√©rifier le statut
                            status = result.get('status', '')
                            if status == 'in-progress':
                                print(f"      ‚è≥ G√©n√©ration en cours... (tentative {attempt}/{max_attempts})")
                                continue
                            elif status == 'complete':
                                print(f"      ‚ùì G√©n√©ration compl√®te mais pas de vid√©o trouv√©e")
                                print(f"      üìã R√©ponse: {result}")
                                return None
                            else:
                                print(f"      ‚ùì Statut inconnu: {status}")
                                print(f"      üìã R√©ponse: {result}")
                        else:
                            # R√©ponse non-JSON, peut-√™tre vid√©o directe
                            video_data = await response.read()
                            if len(video_data) > 1000:
                                video_filename = f"scene_{generation_id}.mp4"
                                video_path = self.clips_dir / video_filename
                                
                                with open(video_path, 'wb') as f:
                                    f.write(video_data)
                                
                                print(f"      ‚úÖ Vid√©o directe t√©l√©charg√©e: {video_filename}")
                                
                                return {
                                    "video_path": str(video_path),
                                    "video_url": f"/cache/animations/clips/{video_filename}",
                                    "generation_id": generation_id
                                }
                    
                    elif response.status == 202:
                        # Toujours en cours de g√©n√©ration
                        print(f"      ‚è≥ G√©n√©ration en cours... (tentative {attempt}/{max_attempts})")
                        continue
                    
                    else:
                        error_text = await response.text()
                        print(f"      ‚ùå Erreur v√©rification: {response.status} - {error_text}")
                        return None
        
        print(f"      ‚è∞ Timeout: g√©n√©ration trop longue")
        return None
    
    async def _generer_clip_demo(self, prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        G√©n√©ration de clip de d√©monstration (remplace SD3-Turbo temporairement)
        """
        scene_id = prompt_data['scene_id']
        
        # Cr√©er un fichier JSON avec les m√©tadonn√©es du clip
        clip_metadata = {
            "scene_id": scene_id,
            "prompt": prompt_data['prompt_text'],
            "duration": prompt_data['duration'],
            "seed": prompt_data.get('seed', 12345),
            "status": "demo_generated",
            "timestamp": time.time()
        }
        
        # Sauvegarder les m√©tadonn√©es
        metadata_path = self.clips_dir / f"scene_{scene_id}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(clip_metadata, f, ensure_ascii=False, indent=2)
        
        # Utiliser l'image SVG de d√©mo existante
        demo_image_path = f"/cache/animations/demo_images/scene_{scene_id}_demo.svg"
        
        return {
            "scene_number": scene_id,
            "scene_id": scene_id,
            "video_path": str(metadata_path),
            "video_url": demo_image_path,  # Pour la compatibilit√© 
            "image_url": demo_image_path,  # URL de l'image pour affichage imm√©diat
            "demo_image_url": demo_image_path,
            "status": "success",
            "statut": "demo_ok",
            "duration": prompt_data['duration'],
            "prompt": prompt_data['prompt_text'],
            "metadata": clip_metadata,
            "type": "local_image"
        }
    
    async def assembler_video_finale(self, clips: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        √âtape 5: Assembler les clips en vid√©o finale MP4
        """
        print("üé¨ √âtape 5: Assemblage vid√©o finale MP4")
        
        # S√©parer les vrais clips vid√©o des d√©mos
        vrais_clips = [c for c in clips if c.get('type') == 'real_video']
        clips_demo = [c for c in clips if c.get('type') != 'real_video']
        
        if vrais_clips:
            # Assembler les vraies vid√©os avec FFmpeg
            video_result = await self._assembler_videos_ffmpeg(clips)
        else:
            # Mode d√©mo : cr√©er une playlist
            video_result = await self._creer_playlist_demo(clips)
        
        duree_totale = sum(clip.get('duration', 20) for clip in clips)
        
        result = {
            "video_finale_path": video_result['path'],
            "video_url": video_result['url'],
            "duree_totale": duree_totale,
            "clips_count": len(clips),
            "format": video_result['format'],
            "has_real_videos": len(vrais_clips) > 0,
            "demo_clips": len(clips_demo)
        }
        
        print(f"‚úÖ Vid√©o finale assembl√©e: {duree_totale}s ({result['format']})")
        return result

    async def _assembler_videos_ffmpeg(self, clips: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Assembler les clips vid√©o en MP4 final avec FFmpeg
        """
        print("      üîß Assemblage FFmpeg...")
        
        import subprocess
        
        # Cr√©er une liste des fichiers vid√©o
        video_files = []
        concat_file = self.final_dir / f"concat_{int(time.time())}.txt"
        
        with open(concat_file, 'w', encoding='utf-8') as f:
            for clip in clips:
                if clip.get('type') == 'real_video' and 'video_path' in clip:
                    video_path = clip['video_path']
                    if os.path.exists(video_path):
                        f.write(f"file '{os.path.abspath(video_path)}'\n")
                        video_files.append(video_path)
        
        if not video_files:
            raise Exception("Aucun fichier vid√©o valide √† assembler")
        
        # Nom du fichier final
        final_video_path = self.final_dir / f"dessin_anime_{int(time.time())}.mp4"
        
        # Commande FFmpeg pour concatener avec transitions douces
        ffmpeg_cmd = [
            'ffmpeg', '-y',  # -y pour overwrite
            '-f', 'concat',
            '-safe', '0',
            '-i', str(concat_file),
            '-c:v', 'libx264',  # Codec vid√©o
            '-preset', 'medium',  # Qualit√©/vitesse
            '-crf', '23',  # Qualit√© (lower = better)
            '-c:a', 'aac',  # Codec audio si pr√©sent
            '-movflags', '+faststart',  # Optimisation web
            str(final_video_path)
        ]
        
        try:
            # Ex√©cuter FFmpeg
            result = subprocess.run(ffmpeg_cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=300)  # 5 minutes max
            
            if result.returncode == 0:
                print(f"      ‚úÖ Vid√©o assembl√©e: {final_video_path.name}")
                
                # Nettoyer le fichier concat temporaire
                concat_file.unlink(missing_ok=True)
                
                return {
                    "path": str(final_video_path),
                    "url": f"/cache/animations/final/{final_video_path.name}",
                    "format": "mp4"
                }
            else:
                print(f"      ‚ùå Erreur FFmpeg: {result.stderr}")
                raise Exception(f"FFmpeg failed: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            print("      ‚è∞ Timeout FFmpeg")
            raise Exception("Assemblage vid√©o trop long")
        except FileNotFoundError:
            print("      ‚ùå FFmpeg non trouv√©")
            # Fallback vers playlist
            return await self._creer_playlist_demo(clips)

    async def _creer_playlist_demo(self, clips: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Cr√©er une playlist de d√©monstration
        """
        print("      üìã Cr√©ation playlist d√©mo...")
        
        playlist_data = {
            "clips": clips,
            "format": "demo_playlist",
            "timestamp": time.time(),
            "note": "Mode d√©monstration - utilisez des vraies vid√©os pour un MP4 final"
        }
        
        playlist_path = self.final_dir / f"playlist_{int(time.time())}.m3u8"
        with open(playlist_path, 'w', encoding='utf-8') as f:
            json.dump(playlist_data, f, ensure_ascii=False, indent=2)
        
        return {
            "path": str(playlist_path),
            "url": f"/cache/animations/final/{playlist_path.name}",
            "format": "playlist"
        }
        
        return {
            "video_finale_path": str(playlist_path),
            "video_url": f"/cache/animations/final/{playlist_path.name}",
            "duree_totale": duree_totale,
            "clips_count": len(clips),
            "format": "demo_playlist"
        }
    
    async def generer_dessin_anime_complet(self, histoire: str, duree_totale: int = 60) -> Dict[str, Any]:
        """
        Pipeline complet: histoire ‚Üí dessin anim√©
        """
        print(f"\nüöÄ D√âBUT G√âN√âRATION DESSIN ANIM√â")
        print(f"üìñ Histoire: {histoire[:100]}...")
        print(f"‚è±Ô∏è Dur√©e cible: {duree_totale}s")
        
        start_time = time.time()
        
        try:
            # √âtape 1: D√©coupage en sc√®nes
            scenes = await self.decouper_histoire_en_scenes(histoire, duree_totale)
            
            # √âtape 2: Style et seeds
            style = await self.definir_style_et_seeds(scenes)
            
            # √âtape 3: Prompts vid√©o
            prompts = await self.generer_prompts_videos(scenes, style)
            
            # √âtape 4: G√©n√©ration clips
            clips = await self.generer_clips_video(prompts)
            
            # √âtape 5: Assemblage final
            video_finale = await self.assembler_video_finale(clips)
            
            generation_time = time.time() - start_time
            
            # Pr√©parer le r√©sultat dans le format attendu par le frontend
            result = {
                "status": "success",
                "message": "‚úÖ Animation g√©n√©r√©e avec succ√®s",
                "timeline": {
                    "total_duration": duree_totale,
                    "scenes_count": len(scenes),
                    "clips_count": len(clips)
                },
                "clips": clips,  # Les clips contiennent d√©j√† les image_url
                "video_finale": video_finale,
                "generation_time": generation_time,
                "pipeline_version": "v2.0_specs",
                
                # M√©tadonn√©es additionnelles pour compatibilit√©
                "scenes": scenes,
                "style": style,
                "histoire": histoire,
                "duree_totale": duree_totale
            }
            
            print(f"\nüéâ G√âN√âRATION TERMIN√âE en {generation_time:.1f}s")
            return result
            
        except Exception as e:
            print(f"\nüí• ERREUR PIPELINE: {e}")
            raise


# Fonction utilitaire pour l'int√©gration
async def creer_dessin_anime(histoire: str, duree: int, openai_key: str, stability_key: str, mode: str = "demo") -> Dict[str, Any]:
    """
    Fonction d'entr√©e principale pour l'API
    
    Args:
        histoire: Texte de l'histoire
        duree: Dur√©e cible en secondes
        openai_key: Cl√© API OpenAI
        stability_key: Cl√© API Stability AI
        mode: "demo" pour mode test avec images, "production" pour vraies vid√©os
    """
    pipeline = DessinAnimePipeline(openai_key, stability_key)
    pipeline.mode = mode  # D√©finir le mode
    return await pipeline.generer_dessin_anime_complet(histoire, duree)


if __name__ == "__main__":
    # Test du pipeline
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    async def test_pipeline():
        openai_key = os.getenv("OPENAI_API_KEY")
        stability_key = os.getenv("STABILITY_API_KEY")
        
        if not openai_key or not stability_key:
            print("‚ùå Cl√©s API manquantes")
            return
        
        histoire = "Une petite licorne d√©couvre un jardin magique plein de fleurs qui chantent et de papillons color√©s qui dansent au rythme de la musique du vent."
        
        result = await creer_dessin_anime(histoire, 90, openai_key, stability_key)
        
        print("\nüìä R√âSULTAT:")
        print(f"‚úÖ Status: {result['status']}")
        print(f"üé¨ Sc√®nes: {len(result['scenes'])}")
        print(f"üé• Clips: {len(result['clips'])}")
        print(f"‚è±Ô∏è Dur√©e: {result['duree_totale']}s")
        print(f"üïê Temps: {result['generation_time']:.1f}s")
    
    asyncio.run(test_pipeline())
